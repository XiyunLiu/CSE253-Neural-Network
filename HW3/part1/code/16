Train on 4096 samples, validate on 4096 samples
Epoch 1/20
4096/4096 [==============================] - 81s - loss: 3.8339 - acc: 0.2957 - val_loss: 2.2533 - val_acc: 0.4937
Epoch 2/20
4096/4096 [==============================] - 83s - loss: 0.8774 - acc: 0.7847 - val_loss: 1.9407 - val_acc: 0.5574
Epoch 3/20
4096/4096 [==============================] - 90s - loss: 0.4238 - acc: 0.8972 - val_loss: 1.8641 - val_acc: 0.5764
Epoch 4/20
4096/4096 [==============================] - 90s - loss: 0.2102 - acc: 0.9526 - val_loss: 1.8527 - val_acc: 0.5908
Epoch 5/20
4096/4096 [==============================] - 89s - loss: 0.1063 - acc: 0.9795 - val_loss: 1.8598 - val_acc: 0.5972
Epoch 6/20
4096/4096 [==============================] - 89s - loss: 0.0559 - acc: 0.9924 - val_loss: 1.8698 - val_acc: 0.6035
Epoch 7/20
4096/4096 [==============================] - 84s - loss: 0.0313 - acc: 0.9971 - val_loss: 1.8784 - val_acc: 0.6082
Epoch 8/20
4096/4096 [==============================] - 83s - loss: 0.0194 - acc: 0.9985 - val_loss: 1.8846 - val_acc: 0.6108
Epoch 9/20
4096/4096 [==============================] - 82s - loss: 0.0135 - acc: 0.9988 - val_loss: 1.8881 - val_acc: 0.6155
Epoch 10/20
4096/4096 [==============================] - 84s - loss: 0.0104 - acc: 0.9990 - val_loss: 1.8911 - val_acc: 0.6169
Epoch 11/20
4096/4096 [==============================] - 82s - loss: 0.0086 - acc: 0.9990 - val_loss: 1.8943 - val_acc: 0.6191
Epoch 12/20
4096/4096 [==============================] - 83s - loss: 0.0075 - acc: 0.9993 - val_loss: 1.8978 - val_acc: 0.6206
Epoch 13/20
4096/4096 [==============================] - 83s - loss: 0.0068 - acc: 0.9993 - val_loss: 1.9013 - val_acc: 0.6223
Epoch 14/20
4096/4096 [==============================] - 84s - loss: 0.0063 - acc: 0.9993 - val_loss: 1.9050 - val_acc: 0.6250
Epoch 15/20
4096/4096 [==============================] - 82s - loss: 0.0060 - acc: 0.9993 - val_loss: 1.9088 - val_acc: 0.6262
Epoch 16/20
4096/4096 [==============================] - 82s - loss: 0.0058 - acc: 0.9993 - val_loss: 1.9126 - val_acc: 0.6274
Epoch 17/20
4096/4096 [==============================] - 85s - loss: 0.0056 - acc: 0.9993 - val_loss: 1.9164 - val_acc: 0.6277
Epoch 18/20
4096/4096 [==============================] - 84s - loss: 0.0055 - acc: 0.9993 - val_loss: 1.9202 - val_acc: 0.6282
Epoch 19/20
4096/4096 [==============================] - 83s - loss: 0.0054 - acc: 0.9993 - val_loss: 1.9239 - val_acc: 0.6289
Epoch 20/20
4096/4096 [==============================] - 83s - loss: 0.0053 - acc: 0.9993 - val_loss: 1.9276 - val_acc: 0.6294

{'acc': [0.295654296875, 0.78466796875, 0.897216796875, 0.95263671875, 0.9794921875, 0.992431640625, 0.9970703125, 0.99853515625, 0.998779296875, 0.9990234375, 0.9990234375, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125], 'loss': [3.8339065322652459, 0.87737164075952023, 0.42384960892377421, 0.21017872382071801, 0.10634218835912179, 0.055873461802548263, 0.031270003280951641, 0.019361631479114294, 0.013522462242690381, 0.010386978488895693, 0.0085830848147452343, 0.0074905256324200309, 0.0067942774112452753, 0.0063286272265941079, 0.0060036300865249359, 0.0057683979184730561, 0.0055927917755980161, 0.0054582444392963225, 0.0053528656587786827, 0.0052687619411244668], 'val_acc': [0.49365234375, 0.557373046875, 0.576416015625, 0.5908203125, 0.59716796875, 0.603515625, 0.608154296875, 0.61083984375, 0.615478515625, 0.616943359375, 0.619140625, 0.62060546875, 0.622314453125, 0.625, 0.626220703125, 0.62744140625, 0.627685546875, 0.628173828125, 0.62890625, 0.62939453125], 'val_loss': [2.2533148173242807, 1.9406881923787296, 1.8641200577840209, 1.8526962930336595, 1.8598414338193834, 1.8697520345449448, 1.8784074722789228, 1.8845616923645139, 1.8881427617743611, 1.8911179597489536, 1.8943235701881349, 1.8977851746603847, 1.9013459612615407, 1.9050148660317063, 1.9087889594957232, 1.912596583366394, 1.9163831318728626, 1.920150485355407, 1.9239003942348063, 1.9275902775116265]}
