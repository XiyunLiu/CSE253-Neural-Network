{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 1080 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import ndimage\n",
    "from skimage import transform\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "import skimage.io as io\n",
    "\n",
    "# MEAN = np.array([123.0, 117.0, 104.0])\n",
    "MEAN = np.array([0, 0, 0])\n",
    "def getModel( output_dim ):\n",
    "    ''' \n",
    "        * output_dim: the number of classes (int)\n",
    "        \n",
    "        * return: compiled model (keras.engine.training.Model)\n",
    "    '''\n",
    "    vgg_model = VGG16( weights='imagenet', include_top=True )\n",
    "    vgg_out = vgg_model.layers[-2].output #Last FC layer's output  \n",
    "    softmax_layer = Dense(256, activation='softmax')(vgg_out) #Create softmax layer taking input as vgg_ou\n",
    "    #Create new transfer learning model\n",
    "    tl_model = Model( input=vgg_model.input, output=softmax_layer )\n",
    "\n",
    "    #Freeze all layers of VGG16 and Compile the model\n",
    "    #Confirm the model is appropriate\n",
    "    for l in vgg_model.layers:\n",
    "        l.trainable = False\n",
    "\n",
    "    return tl_model\n",
    "def convert_image(collect):\n",
    "#     sz = np.shape(collect)[0]\n",
    "    sz = len(collect)\n",
    "    images = np.zeros((sz, 224, 224, 3))\n",
    "    for i in range(sz):\n",
    "        img = collect[i]\n",
    "        img = transform.resize(img, (224, 224))*255\n",
    "        if (len(img.shape) != 3):\n",
    "           img = img[:,:,np.newaxis]\n",
    "        images[i] = img\n",
    "    return images\n",
    "\n",
    "sizeOfEachClass = 32\n",
    "sizeOfTestEachClass = 16\n",
    "path='/home/yiluo/Homework/253/3/256_ObjectCategories/'\n",
    "files = os.listdir(path)\n",
    "X_train, X_val, X_test = [], [], []\n",
    "Y_train, Y_val, Y_test = [], [], []\n",
    "label = 0\n",
    "for category in files:\n",
    "    sub_path = path + category\n",
    "    sub_files = os.listdir(sub_path)\n",
    "    cnt = np.shape(sub_files)[0]\n",
    "    num = 1\n",
    "    for img_nm in sub_files:\n",
    "        if img_nm[-4:] == '.jpg':\n",
    "            img_path = sub_path + '/' + img_nm\n",
    "            #img = ndimage.imread(sub_path + '/' + img_nm)\n",
    "            #img = transform.resize(img, (224, 224))*255\n",
    "            if num <= np.floor(0.8*cnt):\n",
    "                if num <= sizeOfEachClass:\n",
    "                    X_train.append(img_path)\n",
    "                    Y_train.append(label)\n",
    "            elif num <= np.floor(0.9*cnt):\n",
    "                if num <= np.floor(0.8*cnt) + sizeOfTestEachClass:\n",
    "                    X_val.append(img_path)\n",
    "                    Y_val.append(label)\n",
    "            else:\n",
    "                X_test.append(img_path)\n",
    "                Y_test.append(label)\n",
    "            num += 1\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shuffle\n",
    "ind=range(int(len(X_train)))\n",
    "random.shuffle(ind)\n",
    "X_train = [X_train[i] for i in ind]\n",
    "Y_train = [Y_train[i] for i in ind]\n",
    "Y_train_label = np.zeros((len(Y_train), 256))\n",
    "for i in range(len(Y_train)):\n",
    "    Y_train_label[i, Y_train[i]] = 1\n",
    "    \n",
    "#validation  \n",
    "Y_val_label = np.zeros((len(Y_val), 256))\n",
    "for i in range(len(Y_val)):\n",
    "    Y_val_label[i, Y_val[i]] = 1\n",
    "    \n",
    "#test\n",
    "Y_test_label = np.zeros((len(Y_test), 256))\n",
    "for i in range(len(Y_test)):\n",
    "    Y_test_label[i, Y_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Output dim for your dataset\n",
    "output_dim = 256 #For Caltech256\n",
    "\n",
    "#Data\n",
    "coll = io.ImageCollection(X_train)\n",
    "X_train_data = convert_image(coll)\n",
    "# coll_val = io.ImageCollection(X_val)\n",
    "# X_val_data = convert_image(coll_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "tl_model = getModel( output_dim )\n",
    "# rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) # error\n",
    "# sgd = keras.optimizers.SGD(lr=0.0001, momentum=0.9, decay=0.0, nesterov=True)\n",
    "tl_model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    samplewise_center=True)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4096 samples, validate on 4096 samples\n",
      "Epoch 1/20\n",
      "4096/4096 [==============================] - 81s - loss: 3.8339 - acc: 0.2957 - val_loss: 2.2533 - val_acc: 0.4937\n",
      "Epoch 2/20\n",
      "4096/4096 [==============================] - 83s - loss: 0.8774 - acc: 0.7847 - val_loss: 1.9407 - val_acc: 0.5574\n",
      "Epoch 3/20\n",
      "4096/4096 [==============================] - 90s - loss: 0.4238 - acc: 0.8972 - val_loss: 1.8641 - val_acc: 0.5764\n",
      "Epoch 4/20\n",
      "4096/4096 [==============================] - 90s - loss: 0.2102 - acc: 0.9526 - val_loss: 1.8527 - val_acc: 0.5908\n",
      "Epoch 5/20\n",
      "4096/4096 [==============================] - 89s - loss: 0.1063 - acc: 0.9795 - val_loss: 1.8598 - val_acc: 0.5972\n",
      "Epoch 6/20\n",
      "4096/4096 [==============================] - 89s - loss: 0.0559 - acc: 0.9924 - val_loss: 1.8698 - val_acc: 0.6035\n",
      "Epoch 7/20\n",
      "4096/4096 [==============================] - 84s - loss: 0.0313 - acc: 0.9971 - val_loss: 1.8784 - val_acc: 0.6082\n",
      "Epoch 8/20\n",
      "4096/4096 [==============================] - 83s - loss: 0.0194 - acc: 0.9985 - val_loss: 1.8846 - val_acc: 0.6108\n",
      "Epoch 9/20\n",
      "4096/4096 [==============================] - 82s - loss: 0.0135 - acc: 0.9988 - val_loss: 1.8881 - val_acc: 0.6155\n",
      "Epoch 10/20\n",
      "4096/4096 [==============================] - 84s - loss: 0.0104 - acc: 0.9990 - val_loss: 1.8911 - val_acc: 0.6169\n",
      "Epoch 11/20\n",
      "4096/4096 [==============================] - 82s - loss: 0.0086 - acc: 0.9990 - val_loss: 1.8943 - val_acc: 0.6191\n",
      "Epoch 12/20\n",
      "4096/4096 [==============================] - 83s - loss: 0.0075 - acc: 0.9993 - val_loss: 1.8978 - val_acc: 0.6206\n",
      "Epoch 13/20\n",
      "4096/4096 [==============================] - 83s - loss: 0.0068 - acc: 0.9993 - val_loss: 1.9013 - val_acc: 0.6223\n",
      "Epoch 14/20\n",
      "4096/4096 [==============================] - 84s - loss: 0.0063 - acc: 0.9993 - val_loss: 1.9050 - val_acc: 0.6250\n",
      "Epoch 15/20\n",
      "4096/4096 [==============================] - 82s - loss: 0.0060 - acc: 0.9993 - val_loss: 1.9088 - val_acc: 0.6262\n",
      "Epoch 16/20\n",
      "4096/4096 [==============================] - 82s - loss: 0.0058 - acc: 0.9993 - val_loss: 1.9126 - val_acc: 0.6274\n",
      "Epoch 17/20\n",
      "4096/4096 [==============================] - 85s - loss: 0.0056 - acc: 0.9993 - val_loss: 1.9164 - val_acc: 0.6277\n",
      "Epoch 18/20\n",
      "4096/4096 [==============================] - 84s - loss: 0.0055 - acc: 0.9993 - val_loss: 1.9202 - val_acc: 0.6282\n",
      "Epoch 19/20\n",
      "4096/4096 [==============================] - 83s - loss: 0.0054 - acc: 0.9993 - val_loss: 1.9239 - val_acc: 0.6289\n",
      "Epoch 20/20\n",
      "4096/4096 [==============================] - 83s - loss: 0.0053 - acc: 0.9993 - val_loss: 1.9276 - val_acc: 0.6294\n"
     ]
    }
   ],
   "source": [
    "hist = tl_model.fit(X_train_data, Y_train_label, \n",
    "             batch_size=32, nb_epoch=20, \n",
    "             verbose=1, validation_split=0.5,\n",
    "             shuffle=None, class_weight=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': [0.295654296875, 0.78466796875, 0.897216796875, 0.95263671875, 0.9794921875, 0.992431640625, 0.9970703125, 0.99853515625, 0.998779296875, 0.9990234375, 0.9990234375, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125, 0.999267578125], 'loss': [3.8339065322652459, 0.87737164075952023, 0.42384960892377421, 0.21017872382071801, 0.10634218835912179, 0.055873461802548263, 0.031270003280951641, 0.019361631479114294, 0.013522462242690381, 0.010386978488895693, 0.0085830848147452343, 0.0074905256324200309, 0.0067942774112452753, 0.0063286272265941079, 0.0060036300865249359, 0.0057683979184730561, 0.0055927917755980161, 0.0054582444392963225, 0.0053528656587786827, 0.0052687619411244668], 'val_acc': [0.49365234375, 0.557373046875, 0.576416015625, 0.5908203125, 0.59716796875, 0.603515625, 0.608154296875, 0.61083984375, 0.615478515625, 0.616943359375, 0.619140625, 0.62060546875, 0.622314453125, 0.625, 0.626220703125, 0.62744140625, 0.627685546875, 0.628173828125, 0.62890625, 0.62939453125], 'val_loss': [2.2533148173242807, 1.9406881923787296, 1.8641200577840209, 1.8526962930336595, 1.8598414338193834, 1.8697520345449448, 1.8784074722789228, 1.8845616923645139, 1.8881427617743611, 1.8911179597489536, 1.8943235701881349, 1.8977851746603847, 1.9013459612615407, 1.9050148660317063, 1.9087889594957232, 1.912596583366394, 1.9163831318728626, 1.920150485355407, 1.9239003942348063, 1.9275902775116265]}\n"
     ]
    }
   ],
   "source": [
    "print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
